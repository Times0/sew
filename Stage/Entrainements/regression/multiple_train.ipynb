{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import numbers\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import csv\n",
    "import seaborn as sn\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from colorama import Fore, Back, Style\n",
    "from borb.pdf import Document, X11Color, Page, Alignment, SingleColumnLayout, Paragraph, PDF, TableCell, FlexibleColumnWidthTable\n",
    "from decimal import Decimal\n",
    "from IPython.display import clear_output\n",
    "from scipy.interpolate import splrep, BSpline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est très difficile (voir impossible) de faire un système déterministe (donc reproduisible) avec pytorch, car il y differentes sources d'aléatoire:\n",
    "- RNG de python (random)\n",
    "- RNG de pytorch (pour initialiser les poids des nouvelles couches des modèles et pour le shuffle)\n",
    "- Le GPU (le GPU optimise sont algorithme à chaque fois ce qui change les résultats)\n",
    "- RNG numpy\n",
    "- Algorithmes de pytorch non déterministes\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dataset' # name of dataset (with subfolder 'train', 'val', 'test')\n",
    "seed_results_dir = 'Seed_Results_mobile_base_notime' # name of main directory with results\n",
    "bias_init = True\n",
    "bias_init_value = 7.33 # Taille moyenne des rangées\n",
    "\n",
    "base_model = 'mobile' # in 'res', 'efficient', 'mobile'\n",
    "\n",
    "# Graines, si False, les RNG du modèle et du shuffle prendrons les valeurs de seeds\n",
    "Same_model = True\n",
    "Model_gen_seed = 0 # graine de génération pour générer les modèles (pour avoir les mêmes poids initiaux)\n",
    "\n",
    "Same_shuffle = False\n",
    "Shuffle_seed = 0 # graine pour avoir le même shuffle (ordre des images pour l'entrainement)\n",
    "\n",
    "Time_limit = None # None ou int/float, en heures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pdf = 'results_RMSprop.pdf'\n",
    "# Seed\n",
    "seeds = [1] # int: seeds a tester\n",
    "#for i in range(11):\n",
    "#    seeds+=[i]\n",
    "\n",
    "params = {}\n",
    "# params optimizer\n",
    "params['optimizer'] = 'RMSprop'                 # str: 'SGD', 'Adadelta', 'Adam', 'AdamW', 'NAdam', 'RAdam', 'RMSprop', ('LBFGS', 'Adagrad' error)\n",
    "params['lr'] = 0.001                            # float: All\n",
    "params['weight_decay'] = 0                      # float: All other than LBFGS\n",
    "params['epsilon'] = 1.0e-06                     # float: Adadelta, Adagrad, Adam, AdamW, NAdam, RAdam, RMSprop\n",
    "params['rho'] = None                            # float: Adadelta, Adagrad\n",
    "params['initial_accumulator_value'] = None      # float: Adagrad\n",
    "params['betas'] = None                          # [float, float]: Adam, AdamW, NAdam, RAdam\n",
    "params['max_iter'] = None                       # int: LBFGS\n",
    "params['momentum_decay'] = None                 # float: NAdam\n",
    "params['alpha'] = 0                             # float: RMSprop\n",
    "params['momentum'] = 0                          # float: SGD, RMSprop\n",
    "\n",
    "# params criterion\n",
    "params['loss_function'] = 'Huber'               # str: 'MAE', 'MSE', 'Huber'\n",
    "params['delta'] = 3                             # float: Huber\n",
    "\n",
    "# autres hyper paramètres\n",
    "params['imgsz'] = 224                           # int or [int, int]: (w,h), minimum 224 (pour resnet)\n",
    "params['batch_size'] = 64                       # int: maximum 256 avec resnet18 modifier avec 3 couches FC (limite VRAM GPU)\n",
    "params['epochs'] = 3                            # int\n",
    "params['freeze'] = False                        # bool\n",
    "params['earlystop'] = 200                       # None ou int\n",
    "\n",
    "params['dataset'] = data_dir\n",
    "if isinstance(params['imgsz'], int):\n",
    "    params['imgsz']=[params['imgsz'],params['imgsz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_model_resnet(freeze, bias_init_value = 0):\n",
    "    \"\"\"Génère un model basé sur ResNet18 avec des poids pré-entrainés\n",
    "\n",
    "    Args:\n",
    "        freeze (bool): Booléen dictant le gel ou non des couches de convolution du modèle (gel = pas de modifications des poids pendant l'entrainement)\n",
    "        bias_init_value (float, optional): valeur de bias initial pour la dernière couche. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Le modèle\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "    #modification de la dernière couche\n",
    "    model.fc = nn.Sequential( \n",
    "        nn.Linear(512, 1)\n",
    "    )\n",
    "\n",
    "    if bias_init:\n",
    "        with torch.no_grad():\n",
    "            model.fc[-1].bias = nn.Parameter(torch.full(model.fc[-1].bias.shape, bias_init_value))\n",
    "\n",
    "    # gel ou non du reste\n",
    "    if freeze:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_model_efficientnet(freeze, bias_init_value = 0):\n",
    "    \"\"\"Génère un model basé sur EfficientNet_b0 avec des poids pré-entrainés\n",
    "\n",
    "    Args:\n",
    "        freeze (bool): Booléen dictant le gel ou non des couches de convolution du modèle (gel = pas de modifications des poids pendant l'entrainement)\n",
    "        bias_init_value (float, optional): valeur de bias initial pour la dernière couche. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Le modèle\n",
    "    \"\"\"\n",
    "    model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "\n",
    "    #modification de la dernière couche\n",
    "    model.classifier[-1] = nn.Linear(1280,1)\n",
    "\n",
    "    if bias_init:\n",
    "        with torch.no_grad():\n",
    "            model.classifier[-1].bias = nn.Parameter(torch.full(model.fc[-1].bias.shape, bias_init_value))\n",
    "\n",
    "    # gel ou non du reste\n",
    "    if freeze:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_model_mobilenet(freeze, bias_init_value = 0):\n",
    "    \"\"\"Génère un model basé sur MobileNet_v3_small avec des poids pré-entrainés\n",
    "\n",
    "    Args:\n",
    "        freeze (bool): Booléen dictant le gel ou non des couches de convolution du modèle (gel = pas de modifications des poids pendant l'entrainement)\n",
    "        bias_init_value (float, optional): valeur de bias initial pour la dernière couche. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Le modèle\n",
    "    \"\"\"\n",
    "    model = models.mobilenet_v3_small(weights='IMAGENET1K_V1')\n",
    "\n",
    "    #modification de la dernière couche\n",
    "    model.classifier[-1] = nn.Linear(1024,1)\n",
    "\n",
    "    if bias_init:\n",
    "        with torch.no_grad():\n",
    "            model.classifier[-1].bias = nn.Parameter(torch.full(model.fc[-1].bias.shape, bias_init_value))\n",
    "\n",
    "    # gel ou non du reste\n",
    "    if freeze:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_model_function(base_model):\n",
    "    \"\"\"Retourne la fonction qui permer de générer un modèle du type choisi\n",
    "\n",
    "    Args:\n",
    "        base_model (str): Nom de l'implémentation du modèle de base pré-entrainé\n",
    "\n",
    "    Returns:\n",
    "        function: Fonction qui sert à générer le modèle\n",
    "    \"\"\"\n",
    "    match base_model:\n",
    "        case 'res':\n",
    "            return gen_new_model_resnet\n",
    "        case 'efficient':\n",
    "            return gen_new_model_efficientnet\n",
    "        case 'mobile':\n",
    "            return gen_new_model_mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_new_model = gen_new_model_function(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithmes GPU deterministes\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TablePrinter(object):\n",
    "    \"Print a list of dicts as a table\"\n",
    "    def __init__(self, fmt, sep=' ', ul=None):\n",
    "        \"\"\"        \n",
    "        @param fmt: list of tuple(heading, key, width)\n",
    "                        heading: str, column label\n",
    "                        key: dictionary key to value to print\n",
    "                        width: int, column width in chars\n",
    "        @param sep: string, separation between columns\n",
    "        @param ul: string, character to underline column label, or None for no underlining\n",
    "        \"\"\"\n",
    "        super(TablePrinter,self).__init__()\n",
    "        self.fmt   = str(sep).join('{lb}{0}:{1}{rb}'.format(key, width, lb='{', rb='}') for _,key,width in fmt)\n",
    "        self.head  = {key:heading for heading,key,_ in fmt}\n",
    "        self.ul    = {key:str(ul)*width for _,key,width in fmt} if ul else None\n",
    "        self.width = {key:width for _,key,width in fmt}\n",
    "\n",
    "    def row(self, data):\n",
    "        return self.fmt.format(**{ k:str(data.get(k,''))[:w] for k,w in self.width.items()})\n",
    "\n",
    "    def __call__(self, dataList):\n",
    "        _r = self.row\n",
    "        res = [_r(data) for data in dataList]\n",
    "        res.insert(0, _r(self.head))\n",
    "        if self.ul:\n",
    "            res.insert(1, _r(self.ul))\n",
    "        return '\\n'.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(params):\n",
    "    \"\"\"Donne le nom du chemin à partir des valeurs des paramètres\n",
    "\n",
    "    Args:\n",
    "        params (Dict): Dictionnaire des paramètres\n",
    "    \"\"\"\n",
    "    hyp_dir = f'hyp_{params['delta']}\\\n",
    "_{params['lr']}\\\n",
    "_{params['weight_decay']}\\\n",
    "_{params['epsilon']}\\\n",
    "_{params['rho']}\\\n",
    "_{params['initial_accumulator_value']}\\\n",
    "_{params['betas']}\\\n",
    "_{params['max_iter']}\\\n",
    "_{params['momentum_decay']}\\\n",
    "_{params['alpha']}\\\n",
    "_{params['momentum']}'\n",
    "\n",
    "    result_dir=\"-\".join([f'earlystop_{params['earlystop']}'\n",
    "                            , f'freeze_{params['freeze']}'\n",
    "                            , f'imgsz_{params['imgsz']}'\n",
    "                            , f'batch_{params['batch_size']}'\n",
    "                            , f'epochs_{params['epochs']}'\n",
    "                            , params['loss_function']\n",
    "                            , params['optimizer']\n",
    "                            , hyp_dir])\n",
    "    return result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"Classe de dataset custom\n",
    "\n",
    "    Attributs:\n",
    "        data_dir (str): Chemin vers les données\n",
    "        transform (torch.Transform): Transformations appliquées aux données\n",
    "        image_files (List[nom_image1, ...]): Liste des noms des fichiers des images\n",
    "        labels (Dict[nom_image1: label, ...]): Dictionnaire de correspondance nom_image-label\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        \"\"\"Init\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Chemin vers les données\n",
    "            split (str, optional): Split du dataset. Defaults to 'train'.\n",
    "            transform (torch.Transform): Transformations appliquées aux données\n",
    "        \"\"\"\n",
    "        self.data_dir = os.path.join(data_dir, split)\n",
    "        self.transform = transform\n",
    "        self.image_files = [file for file in os.listdir(self.data_dir) if file.endswith('.jpg') or file.endswith('.jpeg')]\n",
    "        self.labels = self.parse_json()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Donne la taille du Dataset\n",
    "\n",
    "        Returns:\n",
    "            int: taille du dataset\n",
    "        \"\"\"\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Récupère une donnée et son label associé\n",
    "\n",
    "        Args:\n",
    "            idx (int): index de la donnée\n",
    "\n",
    "        Returns:\n",
    "            Tuple[donnée, label]: Donnée et label\n",
    "        \"\"\"\n",
    "        image_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.labels[self.image_files[idx]]\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def collate_fn(self, batch: List):\n",
    "        \"\"\"Fonction de stacking pour la création d'un batch\n",
    "\n",
    "        Args:\n",
    "            batch (List[Tuple]): Liste des Tuples[Tensor[3,imgsz, imgsz], Tensor[1]] avec les tenseurs de l'image et du label\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor[Tensor[3,imgsz, imgsz]], List[Tensor[1]]]: Batch\n",
    "        \"\"\"\n",
    "        images_zip, labels_zip = zip(*batch)\n",
    "        images_batch = torch.stack(images_zip, dim = 0)\n",
    "        labels_batch = torch.stack(labels_zip, dim = 0)\n",
    "        return (images_batch, labels_batch)\n",
    "    \n",
    "    def parse_json(self):\n",
    "        \"\"\"Analyse le json de correspondance nom_image-label\n",
    "\n",
    "        Returns:\n",
    "            Dict[nom_image1: label, ...]: Dictionnaire des correspondances\n",
    "        \"\"\"\n",
    "        fname = glob('*.json', dir_fd=self.data_dir)[0]\n",
    "        with open(os.path.join(self.data_dir, fname)) as json_f:\n",
    "            labels = json.load(json_f)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(params):\n",
    "    \"\"\"Génère la fonction de perte\n",
    "\n",
    "    Args:\n",
    "        params (Dict): Dictionnaire des paramètres\n",
    "\n",
    "    Returns:\n",
    "        nn.Loss: Fonction de perte\n",
    "    \"\"\"\n",
    "    match params['loss_function']:\n",
    "        case 'MSE':\n",
    "            criterion = nn.MSELoss()\n",
    "        case 'MAE':\n",
    "            criterion = nn.L1Loss()\n",
    "        case 'Huber':\n",
    "            criterion = nn.HuberLoss(delta = params['delta'])\n",
    "    return criterion\n",
    "\n",
    "def get_optimizer(model, params):\n",
    "    \"\"\"Génère l'optimiseur\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modèle\n",
    "        params (Dict): Dictionnaire des paramètres\n",
    "\n",
    "    Returns:\n",
    "        nn.Optim: Optimiseur\n",
    "    \"\"\"\n",
    "    lr = params['lr']\n",
    "    wd = params['weight_decay']\n",
    "    eps = params['epsilon']\n",
    "    rho = params['rho']\n",
    "    init = params['initial_accumulator_value']\n",
    "    betas = params['betas']\n",
    "    max_iter = params['max_iter']\n",
    "    md = params['momentum_decay']\n",
    "    alpha = params['alpha']\n",
    "    momentum = params['momentum']\n",
    "    \n",
    "    match params['optimizer']:\n",
    "        case 'SGD':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "        case 'Adadelta':\n",
    "            optimizer = torch.optim.Adadelta(model.parameters(), lr = lr, rho = rho, eps=eps, weight_decay=wd)\n",
    "        case 'Adagrad':\n",
    "            optimizer = torch.optim.Adagrad(model.parameters(), lr=lr, weight_decay=wd, eps=eps, initial_accumulator_value=init)\n",
    "        case 'RMSprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=wd, eps=eps, alpha=alpha, momentum=momentum)\n",
    "        case 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd, eps=eps, betas=betas)\n",
    "        case 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, eps=eps, betas=betas)\n",
    "        case 'LBFGS':\n",
    "            optimizer = torch.optim.LBFGS(model.parameters(), lr=lr, max_iter=max_iter)\n",
    "        case 'NAdam':\n",
    "            optimizer = torch.optim.NAdam(model.parameters(), lr=lr, weight_decay=wd, eps=eps, betas=betas, momentum_decay=md)\n",
    "        case 'RAdam':\n",
    "            optimizer = torch.optim.RAdam(model.parameters(), lr=lr, weight_decay=wd, eps=eps, betas=betas)\n",
    "    return optimizer\n",
    "\n",
    "def get_dataloaders(params:dict):\n",
    "    \"\"\"Génère le dataloader\n",
    "\n",
    "    Args:\n",
    "        params (dict): Dictionnaire des paramètres\n",
    "\n",
    "    Returns:\n",
    "        torch.Dataloader: Dataloader\n",
    "    \"\"\"\n",
    "    data_dir = params['dataset']\n",
    "    batch_size = params['batch_size']\n",
    "    imgsz  = params['imgsz']\n",
    "    dataloaders = {}\n",
    "    splits = []\n",
    "    for split in ['test', 'val', 'train']:\n",
    "        if split in os.listdir(data_dir):\n",
    "            splits .append(split)\n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "            transforms.Resize((imgsz[1], imgsz[0])), # h,w\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalisation demandée par la documentation resnet\n",
    "        ])           \n",
    "     \n",
    "    for split in splits:\n",
    "        dataset = MyDataset(data_dir = data_dir, split = split, transform = data_transforms)\n",
    "        # shuffle change l'ordre de passage des données\n",
    "        dataloaders[split] = DataLoader(dataset=dataset, shuffle=True, batch_size = batch_size, collate_fn = dataset.collate_fn)\n",
    "    return dataloaders, data_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pdf(results, path):\n",
    "    \"\"\"Génère un PDF avec un tableau récapitulatif\n",
    "\n",
    "    Args:\n",
    "        results (List): Liste des résultats\n",
    "        path (str): Nom du fichier\n",
    "    \"\"\"\n",
    "    # create document\n",
    "    pdf = Document()\n",
    "    \n",
    "    # add page\n",
    "    page = Page(Decimal(1500), Decimal(1684))\n",
    "    m = Decimal(10)\n",
    "    pdf.add_page(page)\n",
    "    layout = SingleColumnLayout(page)\n",
    "    for i in range(0, len(results), 38):\n",
    "        res = results[i:min(i+38, len(results))]\n",
    "        table = FlexibleColumnWidthTable(number_of_rows=len(res), number_of_columns=2)\n",
    "        for name, v in [(k,v) for k,v in res]:\n",
    "            if isinstance(v, str):\n",
    "                table.add(TableCell(Paragraph(text=name, font=\"Helvetica-Bold\", font_size=Decimal(12))))\n",
    "                table.add(TableCell(Paragraph(text=v, font=\"Helvetica-Bold\", font_size=Decimal(12))))\n",
    "                continue\n",
    "            table.add(TableCell(Paragraph(name)))\n",
    "            table.add(            TableCell(\n",
    "                    Paragraph(str(v), horizontal_alignment=Alignment.CENTERED)\n",
    "                ))\n",
    "        table.set_padding_on_all_cells(Decimal(m), Decimal(m), Decimal(m), Decimal(m))\n",
    "        # set border\n",
    "        table.set_border_width_on_all_cells(Decimal(0.2))\n",
    "        layout.add(table)\n",
    "    with open(path, \"wb\") as in_file_handle:\n",
    "        PDF.dumps(in_file_handle, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_list(path: str):\n",
    "    \"\"\"Analyse le json et donne une liste des labels\n",
    "\n",
    "    Args:\n",
    "        path (str): Chemin vers le json\n",
    "\n",
    "    Returns:\n",
    "        List: Liste des labels\n",
    "    \"\"\"\n",
    "    fname = glob('*.json', dir_fd=path)[0]\n",
    "    with open(os.path.join(path, fname)) as json_f:\n",
    "        json_data = json.load(json_f)\n",
    "        labels = list(json_data.values())\n",
    "    return labels\n",
    "\n",
    "def parse_json_dict(path):\n",
    "    \"\"\"Analyse le json et donne un dictionnaire\n",
    "\n",
    "    Args:\n",
    "        path (str): Chemin vers le json\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionnaire des correspondances nom_image_label\n",
    "    \"\"\"\n",
    "    fname = glob('*.json', dir_fd=path)[0]\n",
    "    with open(os.path.join(path, fname)) as json_f:\n",
    "        labels = json.load(json_f)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, targets, model, optimizer, criterion):\n",
    "    \"\"\"Une étape d'entrainement\n",
    "\n",
    "    Args:\n",
    "        inputs (Tensor[Tensor[3,imgsz, imgsz]]): Images en entrée\n",
    "        targets (Tensor[Tensor[1]]): Labels cibles\n",
    "        model (nn.Module): Modèle\n",
    "        optimizer (Torch.Optim): Optimiseur\n",
    "        criterion (nn.Loss): Fonction de perte\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Explosion de gradient\n",
    "\n",
    "    Returns:\n",
    "        float, float, float, int: Métriques\n",
    "    \"\"\"\n",
    "    device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mae_loss = nn.L1Loss()\n",
    "    model.train()\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    try:\n",
    "        predictions = model(inputs)\n",
    "        loss = criterion(predictions.squeeze(), targets)\n",
    "        mae = mae_loss(predictions.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = 0\n",
    "        round_mae = 0\n",
    "        for i in range(len(predictions)):\n",
    "            pred = round(predictions[i].item())\n",
    "            target = int(targets[i].item())\n",
    "            round_mae += abs(target-pred)\n",
    "            if pred == target:\n",
    "                acc +=1\n",
    "        acc /= len(predictions)\n",
    "        round_mae /= len(predictions)\n",
    "    except ValueError as ve:\n",
    "        raise ValueError(ve)\n",
    "    del inputs, targets\n",
    "    torch.cuda.empty_cache()\n",
    "    return acc, loss.item(), mae.item(), round_mae\n",
    "\n",
    "@torch.no_grad\n",
    "def val_step(inputs, targets, model, criterion):\n",
    "    \"\"\"Une étape d'évaluation\n",
    "\n",
    "    Args:\n",
    "        inputs (Tensor[Tensor[3,imgsz, imgsz]]): Images en entrée\n",
    "        targets (Tensor[Tensor[1]]): Labels cibles\n",
    "        model (nn.Module): Modèle\n",
    "        criterion (nn.Loss): Fonction de perte\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Explosion de gradient\n",
    "\n",
    "    Returns:\n",
    "        float, float, float, int: Métriques\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mae_loss = nn.L1Loss()\n",
    "    model.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    acc = 0\n",
    "    round_mae = 0\n",
    "    try:\n",
    "        predictions = model(inputs)\n",
    "        loss = criterion(predictions.squeeze(), targets)\n",
    "        mae = mae_loss(predictions.squeeze(), targets)\n",
    "        for i in range(len(predictions)):\n",
    "            pred = round(predictions[i].item())\n",
    "            target = int(targets[i].item())\n",
    "            round_mae += abs(target-pred)\n",
    "            if pred == target:\n",
    "                acc +=1\n",
    "        acc /= len(predictions)\n",
    "        round_mae /= len(predictions)\n",
    "    except ValueError as ve:\n",
    "        raise ValueError(ve)\n",
    "    \n",
    "    del inputs, targets\n",
    "    torch.cuda.empty_cache()\n",
    "    return acc, loss.item(), mae.item(), round_mae\n",
    "\n",
    "@torch.no_grad\n",
    "def get_confusion_matrix(model, path, transform, margin_view):\n",
    "    \"\"\"Génère une matrice de confusion\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modèle\n",
    "        path (str): Chemin vers les données\n",
    "        transform (torch.Optim): Tra,nsformations\n",
    "        margin_view (int): Marge de valeurs visibles\n",
    "\n",
    "    Returns:\n",
    "        np.Array, np.Array, np.Array: Matrice de confusion, indices prédiction et vérité (label)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    fnames= glob('*.jpg', dir_fd = path)\n",
    "    labels = parse_json_dict(path)\n",
    "    # init matrix\n",
    "    indices_t = np.linspace(1, max(labels.values()), max(labels.values()))\n",
    "    indices_p = np.linspace(1, max(labels.values())+margin_view, max(labels.values())+margin_view)\n",
    "    confusion_matrix = np.zeros((indices_p.size+1, indices_t.size), dtype = np.int16)\n",
    "    # predictions\n",
    "    model.eval()\n",
    "    for fname in fnames:\n",
    "        image = Image.open(os.path.join(path, fname)).convert(\"RGB\")\n",
    "        image = transform(image)\n",
    "        image = torch.unsqueeze(image, dim=0)\n",
    "        image = image.to(device)\n",
    "        pred = model(image)\n",
    "        pred = round(pred[0][0].item())\n",
    "        if pred in indices_p:\n",
    "            confusion_matrix[pred-1][labels[fname]-1]+=1\n",
    "        else:\n",
    "            confusion_matrix[-1][labels[fname]-1]+=1\n",
    "    return confusion_matrix, indices_p, indices_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frormat des prints à chaque epoch\n",
    "fmt = [\n",
    "    ('Epoch', 'epoch', 8),\n",
    "    ('Train Loss', 'train loss', 20),\n",
    "    ('Train Accuracy', 'train acc', 20),\n",
    "    ('Train MAE', 'train mae', 20),\n",
    "    ('Train Round MAE', 'train round mae', 20),\n",
    "    ('Val Loss', 'val loss', 20),\n",
    "    ('Val Accuracy', 'val acc', 20),\n",
    "    ('Val MAE', 'val mae', 20),\n",
    "    ('Val Round MAE', 'val round mae', 20),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_params):\n",
    "    \"\"\"Boucle d'entrainement\n",
    "\n",
    "    Args:\n",
    "        train_params (Dict): Dictionnaire de paramètres\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Explosion de gradient\n",
    "\n",
    "    Returns:\n",
    "        float: Meilleure précision\n",
    "    \"\"\"\n",
    "    model = train_params['model']\n",
    "    num_epochs = train_params['epochs']\n",
    "    earlystop = train_params['earlystop']\n",
    "    results_dir = train_params['results_dir']\n",
    "    dataloaders = train_params['dataloarders']\n",
    "    optimizer = train_params['optimizer']\n",
    "    criterion = train_params['criterion']\n",
    "    data_transforms = train_params['data_transforms']\n",
    "    if os.path.exists(os.path.join(results_dir, 'results.yaml')):\n",
    "        print('Already trained')\n",
    "        with open(os.path.join(results_dir, 'results.yaml'), 'r') as res:\n",
    "            res_data = yaml.safe_load(res)\n",
    "            try:\n",
    "                best_val_accs = res_data['best val acc']\n",
    "            except KeyError:\n",
    "                best_val_accs = 0\n",
    "            training = False\n",
    "    else:\n",
    "        training = True\n",
    "\n",
    "    if training:\n",
    "        print(Fore.CYAN + results_dir)\n",
    "        print(os.path.join(os.getcwd(), results_dir) + Fore.RESET)\n",
    "        # initialisations\n",
    "        average_training_losses = []\n",
    "        average_training_accs = []\n",
    "        average_val_accs = []\n",
    "        average_val_losses = []\n",
    "        average_training_maes = []\n",
    "        average_val_maes = []\n",
    "        average_training_round_maes = []\n",
    "        average_val_round_maes = []\n",
    "        best_val_accs = 0\n",
    "        best_mae_of_best_acc = float('inf')\n",
    "        best_round_mae_of_best_acc = float('inf')\n",
    "        best_epoch = 0\n",
    "        best_mae_of_best_acc_epoch = 0\n",
    "        best_round_mae_of_best_acc_epoch = 0\n",
    "        best_epochs_steps = []\n",
    "        best_mae_of_best_acc_epochs_steps = []\n",
    "        best_round_mae_of_best_acc_epochs_steps = []\n",
    "        best_round_mae_epochs_steps = []\n",
    "        best_round_mae = float('inf')\n",
    "        best_acc_of_round_mae = 0\n",
    "        best_round_mae_epoch = 0\n",
    "        early_stopping = None\n",
    "\n",
    "        start_time = time.time()\n",
    "        with open(os.path.join(results_dir, 'record.tsv'), 'w') as tsvfile:\n",
    "            writer = csv.writer(tsvfile, delimiter = '\\t', lineterminator = '\\n')\n",
    "            writer.writerow(['epoch', 'train_loss', 'train_acc', 'train_mae_loss', 'train_round_mae', 'val_loss', 'val_acc', 'val_mae_loss', 'val_round_mae'])\n",
    "            for epoch in range(num_epochs):\n",
    "                training_losses = []\n",
    "                training_accs = []\n",
    "                training_maes = []\n",
    "                training_round_maes = []\n",
    "                desc = 'Epoch ' + str(epoch) + '/' + str(num_epochs)\n",
    "                for (x_train, y_train) in tqdm(dataloaders['train'], desc = desc):\n",
    "                    try:\n",
    "                        acc, loss, mae, round_mae = train_step(x_train, y_train, model, optimizer, criterion)\n",
    "                    except ValueError as ve:\n",
    "                        print(Fore.RED + ve + Fore.RESET)\n",
    "                        yaml_dict = {'error': str(ve) + ' during train: exploding gradient!'}\n",
    "                        with open(os.path.join(results_dir, 'results.yaml'), 'w') as yamlf:\n",
    "                            yaml.dump(yaml_dict, yamlf, default_flow_style=False, allow_unicode=True)\n",
    "                        return 0\n",
    "                        \n",
    "                    training_losses.append(loss)\n",
    "                    training_accs.append(acc)\n",
    "                    training_maes.append(mae)\n",
    "                    training_round_maes.append(round_mae)\n",
    "                average_training_loss = sum(training_losses) / len(training_losses)\n",
    "                average_training_acc = sum(training_accs) / len(training_accs)\n",
    "                average_training_mae = sum(training_maes) / len(training_maes)\n",
    "                average_training_round_mae = sum(training_round_maes) / len(training_round_maes)\n",
    "                average_training_losses.append(average_training_loss)\n",
    "                average_training_accs.append(average_training_acc)\n",
    "                average_training_maes.append(average_training_mae)\n",
    "                average_training_round_maes.append(average_training_round_mae)\n",
    "\n",
    "                # Evaluation\n",
    "                val_accs = []\n",
    "                val_loss = []\n",
    "                val_maes = []\n",
    "                val_round_maes = []\n",
    "                for x_val, y_val in dataloaders['val']:\n",
    "                    try:\n",
    "                        acc, loss, mae, round_mae = val_step(x_val, y_val, model, criterion)\n",
    "                    except ValueError as ve:\n",
    "                        print(Fore.RED + ve + Fore.RESET)\n",
    "                        yaml_dict = {'error': str(ve) + ' during val: exploding gradient!'}\n",
    "                        with open(os.path.join(results_dir, 'results.yaml'), 'w') as yamlf:\n",
    "                            yaml.dump(yaml_dict, yamlf, default_flow_style=False, allow_unicode=True)\n",
    "                        return 0\n",
    "                    \n",
    "                    val_accs.append(acc)\n",
    "                    val_loss.append(loss)\n",
    "                    val_maes.append(mae)\n",
    "                    val_round_maes.append(round_mae)\n",
    "                average_val_loss = sum(val_loss) / len(val_loss)\n",
    "                average_val_acc = sum(val_accs) / len(val_accs)\n",
    "                average_val_mae = sum(val_maes)/len(val_maes)\n",
    "                average_val_round_mae = sum(val_round_maes)/len(val_round_maes)\n",
    "                average_val_accs.append(average_val_acc)\n",
    "                average_val_losses.append(average_val_loss)\n",
    "                average_val_maes.append(average_val_mae)\n",
    "                average_val_round_maes.append(average_val_round_mae)\n",
    "                # Meilleurs modèles\n",
    "                # précision\n",
    "                if best_val_accs < average_val_acc:\n",
    "                    torch.save(model.state_dict(), os.path.join(results_dir, 'best.pt'))\n",
    "                    best_val_accs = average_val_acc\n",
    "                    best_epoch = epoch\n",
    "                    best_epochs_steps.append(epoch)\n",
    "                    print(Fore.GREEN + f\"New best model: acc = {best_val_accs}\", Fore.RESET)\n",
    "                # précision et mae\n",
    "                if best_val_accs == average_val_acc and best_mae_of_best_acc > average_val_mae:\n",
    "                    torch.save(model.state_dict(), os.path.join(results_dir, 'best_and_mae.pt'))\n",
    "                    best_mae_of_best_acc = average_val_mae\n",
    "                    best_mae_of_best_acc_epoch = epoch\n",
    "                    best_mae_of_best_acc_epochs_steps.append(epoch)\n",
    "                    print(Fore.GREEN + f\"New best mae for best acc model: acc = {best_val_accs}, mae = {best_mae_of_best_acc}\", Fore.RESET)\n",
    "                # précision et round mae\n",
    "                if best_val_accs == average_val_acc and best_round_mae_of_best_acc > average_val_round_mae:\n",
    "                    torch.save(model.state_dict(), os.path.join(results_dir, 'best_and_round_mae.pt'))\n",
    "                    best_round_mae_of_best_acc =  average_val_round_mae\n",
    "                    best_round_mae_of_best_acc_epoch = epoch\n",
    "                    best_round_mae_of_best_acc_epochs_steps.append(epoch)\n",
    "                    print(Fore.GREEN + f\"New best round mae for best acc model: acc = {best_val_accs}, round mae = {best_round_mae_of_best_acc}\", Fore.RESET)\n",
    "                # round mae\n",
    "                if (best_round_mae > average_val_round_mae) or (best_round_mae == average_val_round_mae and best_acc_of_round_mae < average_val_acc):\n",
    "                    torch.save(model.state_dict(), os.path.join(results_dir, 'best_round_mae.pt'))\n",
    "                    best_round_mae = average_val_round_mae\n",
    "                    best_acc_of_round_mae = average_val_acc\n",
    "                    best_round_mae_epoch = epoch\n",
    "                    best_round_mae_epochs_steps.append(epoch)\n",
    "                    print(Fore.GREEN + f\"New best round mae model: round mae = {best_round_mae}, acc = {best_acc_of_round_mae}\", Fore.RESET)\n",
    "                # Early stopping\n",
    "                if earlystop is not None:\n",
    "                    if epoch > max(best_epoch, best_mae_of_best_acc_epoch, best_round_mae_of_best_acc_epoch, best_round_mae_epoch) + earlystop:\n",
    "                        early_stopping = epoch\n",
    "                        break\n",
    "                writer.writerow([epoch, average_training_loss, average_training_acc, average_training_mae, average_training_round_mae, average_val_loss, average_val_acc, average_val_mae, average_val_round_mae])\n",
    "                data = [{'epoch': epoch,\n",
    "                         'train loss': average_training_loss,\n",
    "                         'train mae': average_training_mae,\n",
    "                         'train acc': average_training_acc,\n",
    "                         'train round mae': average_training_round_mae,\n",
    "                         'val loss': average_val_loss,\n",
    "                         'val mae': average_val_mae,\n",
    "                         'val acc': average_val_acc,\n",
    "                         'val round mae': average_val_round_mae\n",
    "                         }]\n",
    "                print(Fore.WHITE + TablePrinter(fmt, ul='=')(data)+'\\n'+'-'*180)\n",
    "\n",
    "                # limite de temps\n",
    "                end_t = time.time()\n",
    "                time_elapsed=end_t - start_time\n",
    "                if Time_limit is not None:\n",
    "                    if str(datetime.timedelta(seconds=time_elapsed)) > str(datetime.timedelta(hours=Time_limit)):\n",
    "                        early_stopping = epoch\n",
    "                        break\n",
    "\n",
    "        end_t = time.time()\n",
    "        time_elapsed=end_t - start_time\n",
    "        legend = ['train', 'val']\n",
    "        print(\"time_elapsed: {}\".format(str(datetime.timedelta(seconds=time_elapsed))))\n",
    "        plt.style.use('dark_background')\n",
    "\n",
    "        fig = plt.figure(layout=\"constrained\", figsize=(20,10))\n",
    "        gs = GridSpec(3, 1, figure=fig, wspace = 0.1)\n",
    "        fig.suptitle('Results', fontsize=16)\n",
    "\n",
    "        ax = fig.add_subplot(gs[0])\n",
    "        intervals = np.arange(len(average_training_losses))\n",
    "        ax.plot(intervals, average_training_losses, 'b')\n",
    "        intervals = np.arange(1,len(average_training_losses))\n",
    "        ax.plot(intervals, average_val_losses[1:], 'g')\n",
    "        ax.set_ylabel(\"loss: \" + criterion._get_name())\n",
    "\n",
    "        ax = fig.add_subplot(gs[1])\n",
    "        intervals = np.arange(len(average_training_accs))\n",
    "        ax.plot(intervals, average_training_maes, 'b')\n",
    "        intervals = np.arange(1,len(average_training_accs))\n",
    "        ax.plot(intervals, average_val_maes[1:], color = 'g')\n",
    "        ax.set_ylabel(\"loss: MAE\")\n",
    "\n",
    "        ax = fig.add_subplot(gs[2])\n",
    "        intervals = np.arange(len(average_training_accs))\n",
    "        ax.plot(intervals, average_training_accs, 'b')\n",
    "        ax.plot(intervals, average_val_accs, color = 'g')\n",
    "        \n",
    "        ax.set_xlabel(\"epoch\")\n",
    "        ax.set_ylabel(\"accuracy\")\n",
    "\n",
    "        fig.legend(legend)\n",
    "        fig.savefig(os.path.join(results_dir, 'results.png'))\n",
    "        plt.close()\n",
    "        if best_val_accs == 0 :\n",
    "            print(\"Evaluation accuracy (best) = \", best_val_accs)\n",
    "        elif best_val_accs > 0.9:\n",
    "            print(Back.GREEN + \"Evaluation accuracy (best) = \", best_val_accs)\n",
    "        elif best_val_accs > 0.8:\n",
    "            print(Back.BLUE + \"Evaluation accuracy (best) = \", best_val_accs)\n",
    "        elif best_val_accs > 0.65:\n",
    "            print('\\033[48;2;255;90;0m' + \"Evaluation accuracy (best) = \", best_val_accs)\n",
    "        else:\n",
    "            print(Back.RED + \"Evaluation accuracy (best) = \", best_val_accs)\n",
    "        print(Back.RESET)\n",
    "\n",
    "        # clear GPU cache memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Sauvegarde des résultats\n",
    "        yaml_dict = {\n",
    "                    '----Best Accuracy-----': '',\n",
    "                    'time': str(datetime.timedelta(seconds=time_elapsed)),\n",
    "                    'early_stopping': early_stopping,\n",
    "                    'best acc epoch': best_epoch,\n",
    "                    'best epochs steps': str(best_epochs_steps),\n",
    "                    'best val acc': best_val_accs,\n",
    "                    'best acc train loss': average_training_losses[best_epoch],\n",
    "                    'best acc train acc': average_training_accs[best_epoch],\n",
    "                    'best acc train mae loss': average_training_maes[best_epoch],\n",
    "                    'best acc val loss': average_val_losses[best_epoch],\n",
    "                    'best acc val mae loss': average_val_maes[best_epoch],\n",
    "                    '-----Best MAE for Best Accuracy----': '',\n",
    "                    'best mae of acc epoch': best_mae_of_best_acc_epoch,\n",
    "                    'best mae of best acc steps': str(best_mae_of_best_acc_epochs_steps),\n",
    "                    'best mae of best acc': best_mae_of_best_acc,\n",
    "                    '-----Best Round MAE for Best Accuracy----': '',\n",
    "                    'best round mae of acc epoch': best_round_mae_of_best_acc_epoch,\n",
    "                    'best round mae of best acc epochs steps': str(best_round_mae_of_best_acc_epochs_steps),\n",
    "                    'best round mae of best acc': best_round_mae_of_best_acc,\n",
    "                    '----Best Round MAE-----': '',\n",
    "                    'best round mae epoch': best_round_mae_epoch,\n",
    "                    'best round mae epochs steps': str(best_round_mae_epochs_steps),\n",
    "                    'best round mae': best_round_mae,\n",
    "                    'best acc of bes round mae': best_acc_of_round_mae\n",
    "                    }\n",
    "        with open(os.path.join(results_dir, 'results.yaml'), 'w') as yamlf:\n",
    "            yaml.dump(yaml_dict, yamlf, default_flow_style=False, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "    # Sauvegarde des matrices de confusion\n",
    "    # charger le meilleur model\n",
    "    model.load_state_dict(torch.load(os.path.join(results_dir, 'best.pt')))\n",
    "\n",
    "    if not os.path.exists(os.path.join(results_dir, 'val_conf_mat.png')):\n",
    "        path = os.path.join('dataset', 'val')\n",
    "        conf_mat, inds_p, inds_t = get_confusion_matrix(model, path, data_transforms,2)\n",
    "        inds_p = np.append(inds_p, 'other')\n",
    "        df_cm = pd.DataFrame(conf_mat, index = [i for i in inds_p],\n",
    "                        columns = [i for i in inds_t])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        plt.ylabel('predicted')\n",
    "        plt.xlabel('ground truth')\n",
    "        plt.title('Val confusion matrix')\n",
    "        plt.savefig(os.path.join(results_dir, 'val_conf_mat.png'))\n",
    "        plt.close()\n",
    "\n",
    "    # train\n",
    "    if not os.path.exists(os.path.join(results_dir, 'train_conf_mat.png')):\n",
    "        path = os.path.join('dataset', 'train')\n",
    "        conf_mat, inds_p, inds_t = get_confusion_matrix(model, path, data_transforms,2)\n",
    "        inds_p = np.append(inds_p, 'other')\n",
    "        df_cm = pd.DataFrame(conf_mat, index = [i for i in inds_p],\n",
    "                        columns = [i for i in inds_t])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        plt.ylabel('predicted')\n",
    "        plt.xlabel('ground truth')\n",
    "        plt.title('Train confusion matrix')\n",
    "        plt.savefig(os.path.join(results_dir, 'train_conf_mat.png'))\n",
    "        plt.close()\n",
    "\n",
    "    return best_val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_set(seed, params:dict):\n",
    "    \"\"\"Entraine un modèle avec un set de paramètres\n",
    "\n",
    "    Args:\n",
    "        seed (int): Graine d'aléatoire\n",
    "        params (dict): Parmètres\n",
    "\n",
    "    Returns:\n",
    "        str, float: Chemin des résultats et meilleure précision\n",
    "    \"\"\"\n",
    "    # Réinitialiser les seeds RNG\n",
    "    if Same_model:\n",
    "        torch.cuda.manual_seed(Model_gen_seed)\n",
    "        torch.manual_seed(Model_gen_seed)\n",
    "    else:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # dossier cible\n",
    "    results_dir=os.path.join(seed_results_dir, get_path(params), f'seed_{seed}')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    # Sauvegarde des paramètres\n",
    "    with open(os.path.join(results_dir, 'args.yaml'), 'w') as yamlf:\n",
    "        yaml.dump(params, yamlf, default_flow_style=False, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "    # Génération du modèle\n",
    "    model = gen_new_model(params['freeze'], bias_init_value)\n",
    "    # seed pour dataloaders\n",
    "    if Same_shuffle:\n",
    "        torch.cuda.manual_seed(Shuffle_seed)\n",
    "        torch.manual_seed(Shuffle_seed)\n",
    "    else:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device) # déplacer le model vers le GPU a faire avant de construire l'optimiseur (pour adagrad)\n",
    "    optimizer = get_optimizer(model, params)\n",
    "    criterion = get_criterion(params)\n",
    "    dataloaders, data_transforms = get_dataloaders(params)\n",
    "    \n",
    "    # Sauvegarde de l'architecture du modèle\n",
    "    model_stats = summary(model, input_size=(params['batch_size'], 3, 224, 224), row_settings=(\"depth\", \"ascii_only\"))\n",
    "    summary_str = str(model_stats)\n",
    "    with open(os.path.join(results_dir, 'model.txt'), 'w') as modelf:\n",
    "        modelf.write(summary_str)\n",
    "\n",
    "    # entrainement\n",
    "    train_params = {\n",
    "        'model': model,\n",
    "        'epochs': params['epochs'],\n",
    "        'earlystop': params['earlystop'],\n",
    "        'results_dir': results_dir,\n",
    "        'dataloarders': dataloaders,\n",
    "        'optimizer': optimizer,\n",
    "        'criterion': criterion,\n",
    "        'data_transforms': data_transforms\n",
    "    }\n",
    "    best_acc = training_loop(train_params)\n",
    "    print(results_dir)\n",
    "    return results_dir, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeedsTrain(params, seeds:list, results_pdf):\n",
    "    \"\"\"Boucle d'entrainement des modèles pour chaque graine\n",
    "\n",
    "    Args:\n",
    "        params (Dict): Paramètres\n",
    "        seeds (list): Liste des graines\n",
    "        results_pdf (str): Chemin du PDF\n",
    "    \"\"\"\n",
    "    # init\n",
    "    results = [('Seed', 'Best Accuracy')]\n",
    "    i = 0 # seulement pour l'affichage\n",
    "    for seed_id, seed in enumerate(seeds):\n",
    "        i+=1\n",
    "        if i%5==0:\n",
    "            clear_output() # otherwise too many outputs\n",
    "        print(Fore.RED + '=========================================================={}/{}=========================================================='.format(seed_id, len(seeds)))\n",
    "        results_dir, best_acc = Test_set(seed, params)\n",
    "        results.append((results_dir, best_acc))\n",
    "    print(results)\n",
    "    gen_pdf(results=results, path=os.path.join(seed_results_dir, results_pdf))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeedsTrain(params, seeds, results_pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
