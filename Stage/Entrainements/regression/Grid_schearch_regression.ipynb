{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import numbers\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import random\n",
    "import csv\n",
    "import seaborn as sn\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import statistics\n",
    "import itertools\n",
    "from colorama import Fore, Back, Style\n",
    "from borb.pdf import Document, X11Color, Page, Alignment, SingleColumnLayoutWithOverflow, Paragraph, PDF, TableCell, FlexibleColumnWidthTable\n",
    "from borb.pdf.canvas.layout.annotation.remote_go_to_annotation import RemoteGoToAnnotation\n",
    "from decimal import Decimal\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramètres généraux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dataset' # name of dataset (with subfolder 'train', 'val', 'test')\n",
    "grid_results_dir = 'Grid_Results' # name of main directory with results fo grd schearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramètres de grille de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pdf = 'results_SGD.pdf'\n",
    "# grille optimiseurs\n",
    "optimizers = ['SGD']                        # in 'SGD', 'Adadelta', 'Adam', 'AdamW', 'NAdam', 'RAdam', 'RMSprop', ('LBFGS', 'Adagrad' error)\n",
    "lrs = [1, 0.1, 0.01,0.001, 1e-4]            # float: All\n",
    "weight_decays = [0, 0.01]                   # float: All other than LBFGS\n",
    "epsilons = [1e-06, 1e-07]                   # float: Adadelta, Adagrad, Adam, AdamW, NAdam, RAdam, RMSprop\n",
    "rhos = [0.9, 0.95]                          # float: Adadelta, Adagrad\n",
    "initial_accumulator_values = [0.01]         # Adagrad\n",
    "betass = [[0.9, 0.999], [0.85, 0.95]]       # [float, float]: Adam, AdamW, NAdam, RAdam\n",
    "max_iters = [20]                            # int: LBFGS\n",
    "momentum_decays = [1e-4]                    # float: NAdam\n",
    "alphas = [0]                                # float: RMSprop\n",
    "momentums = [0, 0.9]                        # float: SGD, RMSprop\n",
    "\n",
    "# grille loss function\n",
    "loss_functions = ['MAE', 'MSE', 'Huber']    # in 'MAE', 'MSE', 'Huber'\n",
    "deltas = [1, 1.35, 2, 3]                    # float: Huber\n",
    "\n",
    "# autres hyper paramètres\n",
    "imgszs = [224]                              # int or [int, int]: (w,h), minimum 224 (pour resnet)\n",
    "batch_sizes = [64]                          # int: maximum 256 avec resnet18 modifier avec 3 couches FC\n",
    "num_epochss = [100]                         # int\n",
    "freezes = [False]                           # bool\n",
    "earlystop = [25]                            # None ou int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult(L):\n",
    "    \"\"\"Fait le produit de la taille des éléments de la liste\n",
    "\n",
    "    Args:\n",
    "        L (list: list): Liste de listes de paramètres\n",
    "\n",
    "    Returns:\n",
    "        int: Nombre de combinaisons possibles pour cette liste de listes\n",
    "    \"\"\"\n",
    "    res = 1\n",
    "    for l in L:\n",
    "        res*=len(l)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinaisons():\n",
    "    \"\"\"Donne le nombre de combinaisons qui serons testées\n",
    "\n",
    "    Returns:\n",
    "        int: Nombre de combinaisons\n",
    "    \"\"\"\n",
    "    nb_combinaisons = len(freezes)*len(batch_sizes)*len(imgszs)*len(num_epochss)\n",
    "    if 'Huber' in loss_functions:\n",
    "        nb_combinaisons *= len(loss_functions)-1+len(deltas)\n",
    "    nb_combinaisons_crit = 0\n",
    "    for crit in optimizers:\n",
    "        match crit:\n",
    "            case 'SGD':\n",
    "                nb_combinaisons_crit += mult(list((lrs, weight_decays, momentums)))\n",
    "            case 'Adadelta':\n",
    "                nb_combinaisons_crit += mult(list((lrs, weight_decays, epsilons, rhos)))\n",
    "            case 'Adagrad':\n",
    "                nb_combinaisons_crit += mult(list((lrs, weight_decays, epsilons, initial_accumulator_values)))\n",
    "            case 'RMSprop':\n",
    "                nb_combinaisons_crit += mult(list((lrs, weight_decays, epsilons, alphas, momentums)))\n",
    "            case 'Adam':\n",
    "                nb_combinaisons_crit += mult(list((lrs, weight_decays, epsilons, betass)))\n",
    "            case 'AdamW':\n",
    "                nb_combinaisons_crit += mult(list((lrs, weight_decays, epsilons, betass)))\n",
    "            case 'LBFGS':\n",
    "                nb_combinaisons_crit += mult(list((lrs, max_iters)))\n",
    "            case 'NAdam':\n",
    "                nb_combinaisons_crit += mult(list((lrs, weight_decays, epsilons, betass, momentum_decays)))\n",
    "            case 'RAdam':\n",
    "                nb_combinaisons_crit += mult(list((lrs, weight_decays, epsilons, betass)))\n",
    "    nb_combinaisons *=nb_combinaisons_crit\n",
    "    return nb_combinaisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_params_sets(optimizer):\n",
    "    \"\"\"Donne un liste des combinainsons de paramètres possibles pour cette optimiseur\n",
    "\n",
    "    Args:\n",
    "        optimizer (str): Optimiseur\n",
    "\n",
    "    Returns:\n",
    "        list: Liste de listes de paramètres d'optimiseur\n",
    "    \"\"\"\n",
    "    match optimizer:\n",
    "        case 'SGD':\n",
    "            params_sets = list(itertools.product(lrs, weight_decays, momentums))\n",
    "        case 'Adadelta':\n",
    "            params_sets = list(itertools.product(lrs, weight_decays, epsilons, rhos))\n",
    "        case 'Adagrad':\n",
    "            params_sets = list(itertools.product(lrs, weight_decays, epsilons, initial_accumulator_values))\n",
    "        case 'RMSprop':\n",
    "            params_sets = list(itertools.product(lrs, weight_decays, epsilons, alphas, momentums))\n",
    "        case 'Adam':\n",
    "            params_sets = list(itertools.product(lrs, weight_decays, epsilons, betass))\n",
    "        case 'AdamW':\n",
    "            params_sets = list(itertools.product(lrs, weight_decays, epsilons, betass))\n",
    "        case 'LBFGS':\n",
    "            params_sets = list(itertools.product(lrs, max_iters))\n",
    "        case 'NAdam':\n",
    "            params_sets = list(itertools.product(lrs, weight_decays, epsilons, betass, momentum_decays))\n",
    "        case 'RAdam':\n",
    "            params_sets = list(itertools.product(lrs, weight_decays, epsilons, betass))\n",
    "    return list(map(list, params_sets))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyper_params_sets(loss_function, optimizer):\n",
    "    \"\"\"Donne une liste de combinaisons d'hyperparamètres pour la fonction de perte et l'optimiseur\n",
    "\n",
    "    Args:\n",
    "        loss_function (str): Fonction de perte\n",
    "        optimizer (str): Optimiseur\n",
    "\n",
    "    Returns:\n",
    "        list: Liste de listes de paramètres pour la function de perte et l'optimiseur\n",
    "    \"\"\"\n",
    "    optim_sets = get_opt_params_sets(optimizer)\n",
    "    if loss_function == 'Huber':\n",
    "        params_sets = list(itertools.product(deltas, optim_sets))\n",
    "        params_sets = list(map(list, params_sets))\n",
    "        params_sets = [[xs[0]]+[x for x in xs[1]] for xs in params_sets]\n",
    "        return params_sets\n",
    "    return list(map(list, optim_sets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyper_params_dict(results_dir, hyper_params_set):\n",
    "    \"\"\"Donne un dictionnaire correspondant a une combinaison d'hyperparamètres\n",
    "\n",
    "    Args:\n",
    "        results_dir (str): Chemin d'accès au dossier de résultats\n",
    "        hyper_params_set (list): Liste des hyperparamètres (la taille change celon la fonction de perte et l'optimiseur dans la combinaison qui n'ont pas tous les mêmes hyperparamètres)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire avec les hyperparamètres\n",
    "    \"\"\"\n",
    "    hyper_params_dict = {\n",
    "        'results_dir': results_dir,\n",
    "        'earlystop': hyper_params_set[0],\n",
    "        'freeze': hyper_params_set[1],\n",
    "        'batch_size': hyper_params_set[3],\n",
    "        'epochs': hyper_params_set[4],\n",
    "        'loss_function': hyper_params_set[5],\n",
    "        'optimizer': hyper_params_set[6],\n",
    "        'dataset': data_dir\n",
    "    }\n",
    "    if isinstance(hyper_params_set[2],int):\n",
    "        imgsz = [hyper_params_set[2],hyper_params_set[2]]\n",
    "    else:\n",
    "        imgsz = hyper_params_set[2]\n",
    "    hyper_params_dict['imgsz'] = imgsz\n",
    "    if hyper_params_set[5] == 'Huber':\n",
    "        hyper_params_dict['delta'] = hyper_params_set[7]\n",
    "        d = 1\n",
    "    else:\n",
    "        hyper_params_dict['delta'] = None\n",
    "        d = 0\n",
    "    match hyper_params_set[6]:\n",
    "        case 'SGD':\n",
    "            hyper_params_dict['lr'] = hyper_params_set[7+d]\n",
    "            hyper_params_dict['weight_decay'] = hyper_params_set[8+d]\n",
    "            hyper_params_dict['epsilon'] = None\n",
    "            hyper_params_dict['rho'] = None\n",
    "            hyper_params_dict['initial_accumulator_value'] = None\n",
    "            hyper_params_dict['betas'] = None\n",
    "            hyper_params_dict['max_iter'] = None\n",
    "            hyper_params_dict['momentum_decay'] = None\n",
    "            hyper_params_dict['alpha'] = None\n",
    "            hyper_params_dict['momentum'] = hyper_params_set[9+d]\n",
    "        case 'Adadelta':\n",
    "            hyper_params_dict['lr'] = hyper_params_set[7+d]\n",
    "            hyper_params_dict['weight_decay'] = hyper_params_set[8+d]\n",
    "            hyper_params_dict['epsilon'] = hyper_params_set[9+d]\n",
    "            hyper_params_dict['rho'] = hyper_params_set[10+d]\n",
    "            hyper_params_dict['initial_accumulator_value'] = None\n",
    "            hyper_params_dict['betas'] = None\n",
    "            hyper_params_dict['max_iter'] = None\n",
    "            hyper_params_dict['momentum_decay'] = None\n",
    "            hyper_params_dict['alpha'] = None\n",
    "            hyper_params_dict['momentum'] = None\n",
    "        case 'Adagrad':\n",
    "            hyper_params_dict['lr'] = hyper_params_set[7+d]\n",
    "            hyper_params_dict['weight_decay'] = hyper_params_set[8+d]\n",
    "            hyper_params_dict['epsilon'] = hyper_params_set[9+d]\n",
    "            hyper_params_dict['rho'] = None\n",
    "            hyper_params_dict['initial_accumulator_value'] = hyper_params_set[10+d]\n",
    "            hyper_params_dict['betas'] = None\n",
    "            hyper_params_dict['max_iter'] = None\n",
    "            hyper_params_dict['momentum_decay'] = None\n",
    "            hyper_params_dict['alpha'] = None\n",
    "            hyper_params_dict['momentum'] = None\n",
    "        case 'RMSprop':\n",
    "            hyper_params_dict['lr'] = hyper_params_set[7+d]\n",
    "            hyper_params_dict['weight_decay'] = hyper_params_set[8+d]\n",
    "            hyper_params_dict['epsilon'] = hyper_params_set[9+d]\n",
    "            hyper_params_dict['rho'] = None\n",
    "            hyper_params_dict['initial_accumulator_value'] = None\n",
    "            hyper_params_dict['betas'] = None\n",
    "            hyper_params_dict['max_iter'] = None\n",
    "            hyper_params_dict['momentum_decay'] = None\n",
    "            hyper_params_dict['alpha'] = hyper_params_set[10+d]\n",
    "            hyper_params_dict['momentum'] = hyper_params_set[11+d]\n",
    "        case 'Adam':\n",
    "            hyper_params_dict['lr'] = hyper_params_set[7+d]\n",
    "            hyper_params_dict['weight_decay'] = hyper_params_set[8+d]\n",
    "            hyper_params_dict['epsilon'] = hyper_params_set[9+d]\n",
    "            hyper_params_dict['rho'] = None\n",
    "            hyper_params_dict['initial_accumulator_value'] = None\n",
    "            hyper_params_dict['betas'] = hyper_params_set[10+d]\n",
    "            hyper_params_dict['max_iter'] = None\n",
    "            hyper_params_dict['momentum_decay'] = None\n",
    "            hyper_params_dict['alpha'] = None\n",
    "            hyper_params_dict['momentum'] = None\n",
    "        case 'AdamW':\n",
    "            hyper_params_dict['lr'] = hyper_params_set[7+d]\n",
    "            hyper_params_dict['weight_decay'] = hyper_params_set[8+d]\n",
    "            hyper_params_dict['epsilon'] = hyper_params_set[9+d]\n",
    "            hyper_params_dict['rho'] = None\n",
    "            hyper_params_dict['initial_accumulator_value'] = None\n",
    "            hyper_params_dict['betas'] = hyper_params_set[10+d]\n",
    "            hyper_params_dict['max_iter'] = None\n",
    "            hyper_params_dict['momentum_decay'] = None\n",
    "            hyper_params_dict['alpha'] = None\n",
    "            hyper_params_dict['momentum'] = None\n",
    "        case 'LBFGS':\n",
    "            hyper_params_dict['lr'] = hyper_params_set[7+d]\n",
    "            hyper_params_dict['weight_decay'] = None\n",
    "            hyper_params_dict['epsilon'] = None\n",
    "            hyper_params_dict['rho'] = None\n",
    "            hyper_params_dict['initial_accumulator_value'] = None\n",
    "            hyper_params_dict['betas'] = None\n",
    "            hyper_params_dict['max_iter'] = hyper_params_set[8+d]\n",
    "            hyper_params_dict['momentum_decay'] = None\n",
    "            hyper_params_dict['alpha'] = None\n",
    "            hyper_params_dict['momentum'] = None\n",
    "        case 'NAdam':\n",
    "            hyper_params_dict['lr'] = hyper_params_set[7+d]\n",
    "            hyper_params_dict['weight_decay'] = hyper_params_set[8+d]\n",
    "            hyper_params_dict['epsilon'] = hyper_params_set[9+d]\n",
    "            hyper_params_dict['rho'] = None\n",
    "            hyper_params_dict['initial_accumulator_value'] = None\n",
    "            hyper_params_dict['betas'] = hyper_params_set[10+d]\n",
    "            hyper_params_dict['max_iter'] = None\n",
    "            hyper_params_dict['momentum_decay'] = hyper_params_set[11+d]\n",
    "            hyper_params_dict['alpha'] = None\n",
    "            hyper_params_dict['momentum'] = None\n",
    "        case 'RAdam':\n",
    "            hyper_params_dict['lr'] = hyper_params_set[7+d]\n",
    "            hyper_params_dict['weight_decay'] = hyper_params_set[8+d]\n",
    "            hyper_params_dict['epsilon'] = hyper_params_set[9+d]\n",
    "            hyper_params_dict['rho'] = None\n",
    "            hyper_params_dict['initial_accumulator_value'] = None\n",
    "            hyper_params_dict['betas'] = hyper_params_set[10+d]\n",
    "            hyper_params_dict['max_iter'] = None\n",
    "            hyper_params_dict['momentum_decay'] = None\n",
    "            hyper_params_dict['alpha'] = None\n",
    "            hyper_params_dict['momentum'] = None\n",
    "    return hyper_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid():\n",
    "    \"\"\"Génère la grille de recherche\n",
    "\n",
    "    Returns:\n",
    "        list: Liste de dictionnaire correspondants aux combinaisons à tester\n",
    "    \"\"\"\n",
    "    grid = []\n",
    "    with tqdm(total=combinaisons(), desc = 'préparation', colour='green')as pbar:\n",
    "        hight_hyper_params = list(itertools.product(earlystop, freezes, imgszs, batch_sizes, num_epochss, loss_functions, optimizers))\n",
    "        hight_hyper_params = list(map(list, hight_hyper_params))\n",
    "        for hhp in hight_hyper_params:\n",
    "            low_hyper_params_sets = get_hyper_params_sets(hhp[5], hhp[6])\n",
    "            hyper_params_sets = list()\n",
    "            for low_hyper_params_set in low_hyper_params_sets:\n",
    "                hyper_params_sets.append([*hhp, *low_hyper_params_set])\n",
    "            for hyper_params_set in hyper_params_sets:\n",
    "                results_dir = grid_results_dir\n",
    "                #earlystop\n",
    "                results_dir = os.path.join(results_dir, f'earlystop_{hyper_params_set[0]}')\n",
    "                # freeze\n",
    "                if hyper_params_set[1]:\n",
    "                    results_dir = os.path.join(results_dir, 'freeze')\n",
    "                else:\n",
    "                    results_dir = os.path.join(results_dir, 'no_freeze')\n",
    "                # image size\n",
    "                if isinstance(hyper_params_set[2], int):\n",
    "                    imgsz_dir = 'imgsz_' + str(hyper_params_set[2]) + '-' + str(hyper_params_set[2])\n",
    "                else:\n",
    "                    imgsz_dir = 'imgsz_' + str(hyper_params_set[2][0]) + '-' + str(hyper_params_set[2][1])\n",
    "                results_dir = os.path.join(results_dir, imgsz_dir)\n",
    "                # nom batch\n",
    "                batch_dir = 'batch_' + str(hyper_params_set[3])\n",
    "                # nom epochs\n",
    "                epochs_dir = 'epochs_' + str(hyper_params_set[4])\n",
    "                # join batch, epochs, criterion\n",
    "                results_dir = os.path.join(results_dir, batch_dir, epochs_dir, hyper_params_set[5])\n",
    "                # ajoute subfolder si huber loss\n",
    "                if hyper_params_set[5] == 'Huber':\n",
    "                    delta_dir = 'delta_' + str(hyper_params_set[7])\n",
    "                    results_dir = os.path.join(results_dir, delta_dir)\n",
    "                # optimiseur\n",
    "                results_dir = os.path.join(results_dir, hyper_params_set[6])\n",
    "                # lr\n",
    "                results_dir = os.path.join(results_dir, 'lr_{}'.format(hyper_params_set[8 if hyper_params_set[5] == 'Huber' else 7]))\n",
    "                #low hyp name\n",
    "                low_dir = 'hyp'\n",
    "                for i in range(9 if hyper_params_set[5] == 'Huber' else 8, len(hyper_params_set)):\n",
    "                    if isinstance(hyper_params_set[i],(int, float, str)):\n",
    "                        low_dir += '_' + str(hyper_params_set[i])\n",
    "                    else:\n",
    "                        low_dir += '_' + str(hyper_params_set[i][0])+'-'+str(hyper_params_set[i][1])\n",
    "                results_dir = os.path.join(results_dir, low_dir)\n",
    "                hyper_params_dict = get_hyper_params_dict(results_dir, hyper_params_set)\n",
    "                grid.append(hyper_params_dict)\n",
    "                pbar.update(1)\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_model(freeze):\n",
    "    \"\"\"Génère un nouveau modèles de regression\n",
    "\n",
    "    Args:\n",
    "        freeze (bool): Booléen pour savoir si le modèle doit figer ses poids (sauf les nouvelles couches modifiées)\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Modèle de regression\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "    #modification de la dernière couche\n",
    "    model.fc = nn.Sequential( \n",
    "        nn.Linear(512, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 1)\n",
    "    )\n",
    "\n",
    "    # gel ou non du reste\n",
    "    if freeze:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_list(path):\n",
    "    \"\"\"Analyse le fichier d'annotations\n",
    "\n",
    "    Args:\n",
    "        path (str): Chemin vers le fichier d'annotations (json)\n",
    "\n",
    "    Returns:\n",
    "        list: Liste d'entier correspondants aux valeurs des annotations\n",
    "    \"\"\"\n",
    "    fname = glob('*.json', dir_fd=path)[0]\n",
    "    with open(os.path.join(path, fname)) as json_f:\n",
    "        json_data = json.load(json_f)\n",
    "        labels = list(json_data.values())\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_dict(path):\n",
    "    \"\"\"Analyse le fichier d'annotations\n",
    "\n",
    "    Args:\n",
    "        path (str): Chemin vers le fichier d'annotations (json)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire avec la correspondance image: annotation\n",
    "    \"\"\"\n",
    "    fname = glob('*.json', dir_fd=path)[0]\n",
    "    with open(os.path.join(path, fname)) as json_f:\n",
    "        labels = json.load(json_f)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataset(path):\n",
    "    \"\"\"Analyse le jeu de donné\n",
    "\n",
    "    Args:\n",
    "        path (str): Chemin vers le fichier d'annotations (json)\n",
    "    \"\"\"\n",
    "    colors = ['red', 'lime']\n",
    "    labels = ['train', 'val']\n",
    "    data_train = parse_json_list(os.path.join(path, 'train'))\n",
    "    data_val = parse_json_list(os.path.join(path, 'val'))\n",
    "    bins = np.linspace(min(data_train), max(data_train), max(data_train))\n",
    "    plt.style.use('ggplot')\n",
    "    plt.hist((data_train, data_val), bins, color = colors, label = labels)\n",
    "    plt.legend(prop={'size': 10})\n",
    "    plt.title('label repartition')\n",
    "    plt.xlabel('number of boxes on image')\n",
    "    plt.ylabel('instances')\n",
    "    plt.show()\n",
    "    print('moyenne train: ', statistics.mean(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(image, imgsz):    \n",
    "    w, h = image.size\n",
    "    w_padding = max((imgsz[0] - w) / 2, 0)\n",
    "    h_padding = max((imgsz[1] - h) / 2, 0)\n",
    "    t_pad = h_padding if h_padding % 1 == 0 else h_padding+0.5\n",
    "    l_pad = w_padding if w_padding % 1 == 0 else w_padding+0.5\n",
    "    b_pad = h_padding if h_padding % 1 == 0 else h_padding-0.5\n",
    "    r_pad = w_padding if w_padding % 1 == 0 else w_padding-0.5\n",
    "    padding = (int(l_pad), int(t_pad), int(r_pad), int(b_pad))\n",
    "    return padding\n",
    "\n",
    "class NewPad(object):\n",
    "    def __init__(self, fill=0, padding_mode='constant', imgsz = [224,224]):\n",
    "        assert isinstance(fill, (numbers.Number, str, tuple))\n",
    "        assert padding_mode in ['constant', 'edge', 'reflect', 'symmetric']\n",
    "        self.imgsz = imgsz\n",
    "        self.fill = fill\n",
    "        self.padding_mode = padding_mode\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be padded.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Padded image.\n",
    "        \"\"\"\n",
    "        return F.pad(img, get_padding(img, self.imgsz), self.fill, self.padding_mode)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(padding={0}, fill={1}, padding_mode={2})'.\\\n",
    "            format(self.fill, self.padding_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        self.data_dir = os.path.join(data_dir, split)\n",
    "        self.transform = transform\n",
    "        self.image_files = [file for file in os.listdir(self.data_dir) if file.endswith('.jpg') or file.endswith('.jpeg')]\n",
    "        self.labels = self.parse_json()\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.labels[self.image_files[idx]]\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        images_zip, labels_zip = zip(*batch)\n",
    "        images_batch = torch.stack(images_zip, dim = 0)\n",
    "        labels_batch = torch.stack(labels_zip, dim = 0)\n",
    "        return (images_batch, labels_batch)\n",
    "    \n",
    "    def parse_json(self):\n",
    "        fname = glob('*.json', dir_fd=self.data_dir)[0]\n",
    "        with open(os.path.join(self.data_dir, fname)) as json_f:\n",
    "            labels = json.load(json_f)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs, targets, model, optimizer, criterion):\n",
    "    \"\"\"Une étape d'entraînement (un batch)\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): Batch de données\n",
    "        targets (torch.Tensor): Valeurs cibles\n",
    "        model (nn.Module): Modèle de regression\n",
    "        optimizer (Optimizer): Optimiseur\n",
    "        criterion (Loss Function): Fonction de perte\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Erreur si explosion du gradient\n",
    "\n",
    "    Returns:\n",
    "        acc(float): la précision du model, nombre de prédictions exactement juste/ nombre de prédictions\n",
    "        loss.item()(float): valeur de perte (décalage entre prédiction et réalité), dépend de la fonction choisie (valeur absolue, quadratique, huber)\n",
    "        mae.item()(float): valeur de perte (fonction MAE) c'est la valeur abolue de décalage\n",
    "    \"\"\"\n",
    "    device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mae_loss = nn.L1Loss()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    try:\n",
    "        predictions = model(inputs)\n",
    "        loss = criterion(predictions.squeeze(), targets)\n",
    "        mae = mae_loss(predictions.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = 0\n",
    "        for i in range(len(predictions)):\n",
    "            pred = round(predictions[i].item())\n",
    "            target = int(targets[i].item())\n",
    "            if pred == target:\n",
    "                acc +=1\n",
    "        acc /= len(predictions)\n",
    "    except ValueError as ve:\n",
    "        raise ValueError(ve)\n",
    "    \n",
    "    # nettoyage avant sortie\n",
    "    del inputs, targets\n",
    "    torch.cuda.empty_cache()\n",
    "    return acc, loss.item(), mae.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(inputs, targets, model, criterion):\n",
    "    \"\"\"Une étape d'entraînement (un batch)\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): Batch de données\n",
    "        targets (torch.Tensor): Valeurs cibles\n",
    "        model (nn.Module): Modèle de regression\n",
    "        criterion (Loss Function): Fonction de perte\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Erreur si explosion du gradient\n",
    "\n",
    "    Returns:\n",
    "        acc(float): la précision du model, nombre de prédictions exactement juste/ nombre de prédictions\n",
    "        loss.item()(float): valeur de perte (décalage entre prédiction et réalité), dépend de la fonction choisie (valeur absolue, quadratique, huber)\n",
    "        mae.item()(float): valeur de perte (fonction MAE) c'est la valeur abolue de décalage\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    mae_loss = nn.L1Loss()\n",
    "    model.eval()\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    acc = 0\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            predictions = model(inputs)\n",
    "            loss = criterion(predictions.squeeze(), targets)\n",
    "            mae = mae_loss(predictions.squeeze(), targets)\n",
    "            for i in range(len(predictions)):\n",
    "                pred = round(predictions[i].item())\n",
    "                target = int(targets[i].item())\n",
    "                if pred == target:\n",
    "                    acc +=1\n",
    "        acc /= len(predictions)\n",
    "    except ValueError as ve:\n",
    "        raise ValueError(ve)\n",
    "    \n",
    "    # nettoyage avant sortie\n",
    "    del inputs, targets\n",
    "    torch.cuda.empty_cache()\n",
    "    return acc, loss.item(), mae.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(model, path, transform):\n",
    "    \"\"\"Donne la matrice de confusion\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modèle de regression\n",
    "        path (str): Chemin vers le fichier d'annotations\n",
    "        transform (torchvision.transforms): Outil de pré-traitement des images\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matrice de confusion\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    fnames= glob('*.jpg', dir_fd = path)\n",
    "    labels = parse_json_dict(path)\n",
    "    # init matrix\n",
    "    indices_t = np.linspace(1, max(labels.values()), max(labels.values()))\n",
    "    indices_p = np.linspace(1, 40, 40)\n",
    "    confusion_matrix = np.zeros((indices_p.size+1, indices_t.size), dtype = np.int16)\n",
    "    # predictions\n",
    "    model.eval()\n",
    "    for fname in fnames:\n",
    "        image = Image.open(os.path.join(path, fname)).convert(\"RGB\")\n",
    "        image = transform(image)\n",
    "        image = torch.unsqueeze(image, dim=0)\n",
    "        image = image.to(device)\n",
    "        pred = model(image)\n",
    "        pred = round(pred[0][0].item())\n",
    "        if pred in indices_p:\n",
    "            confusion_matrix[pred-1][labels[fname]-1]+=1\n",
    "        else:\n",
    "            confusion_matrix[-1][labels[fname]-1]+=1\n",
    "    return confusion_matrix, indices_p, indices_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_params):\n",
    "    \"\"\"Entraine le modèle\n",
    "\n",
    "    Args:\n",
    "        train_params (dict): Dictionnaire contenant les hyperparamètres sassociées à l'entrainement\n",
    "\n",
    "    Do:\n",
    "        Sauvegarde le meilleur modèle \"best.pt\"\n",
    "        Sauvegarde l'évolution des résultats \"record.tsv\" et son graphe \"results.png\"\n",
    "        Sauvegarde les résultats finaux \"resultats.yaml\"\n",
    "        Génère et sauvegarde les matrice de confusions (train, val)\n",
    "\n",
    "    Returns:\n",
    "        float: La valeur de la meilleure précision (des meilleurs poids)\n",
    "    \"\"\"\n",
    "    model = train_params['model']\n",
    "    num_epochs = train_params['epochs']\n",
    "    earlystop = train_params['earlystop']\n",
    "    results_dir = train_params['results_dir']\n",
    "    dataloaders = train_params['dataloarders']\n",
    "    optimizer = train_params['optimizer']\n",
    "    criterion = train_params['criterion']\n",
    "    data_transforms = train_params['data_transforms']\n",
    "    print(Fore.CYAN + results_dir)\n",
    "    print(os.path.join(os.getcwd(), results_dir) + Fore.RESET)\n",
    "    # initialisations\n",
    "    average_training_losses = []\n",
    "    average_training_accs = []\n",
    "    average_val_accs = []\n",
    "    average_val_losses = []\n",
    "    average_training_maes = []\n",
    "    average_val_maes = []\n",
    "    best_val_accs = 0\n",
    "    best_epoch = 0\n",
    "    early_stopping = None\n",
    "\n",
    "    start_time = time.time()\n",
    "    with open(os.path.join(results_dir, 'record.tsv'), 'w') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter = '\\t', lineterminator = '\\n')\n",
    "        writer.writerow(['epoch', 'train_loss', 'train_mae_loss', 'train_acc', 'val_loss', 'val_mae_loss', 'val_acc'])\n",
    "        for epoch in range(num_epochs):\n",
    "            training_losses = []\n",
    "            training_accs = []\n",
    "            training_maes = []\n",
    "            desc = 'Epoch ' + str(epoch) + '/' + str(num_epochs)\n",
    "            for (x_train, y_train) in tqdm(dataloaders['train'], desc = desc):\n",
    "                try:\n",
    "                    acc, loss, mae = train_step(x_train, y_train, model, optimizer, criterion)\n",
    "                except ValueError as ve:\n",
    "                    print(Fore.RED + ve + Fore.RESET)\n",
    "                    yaml_dict = {'error': str(ve) + ' during train: exploding gradient!'}\n",
    "                    with open(os.path.join(results_dir, 'results.yaml'), 'w') as yamlf:\n",
    "                        yaml.dump(yaml_dict, yamlf, default_flow_style=False, allow_unicode=True)\n",
    "                    return 0\n",
    "                    \n",
    "                training_losses.append(loss)\n",
    "                training_accs.append(acc)\n",
    "                training_maes.append(mae)\n",
    "            average_training_loss = sum(training_losses) / len(training_losses)\n",
    "            average_training_acc = sum(training_accs) / len(training_accs)\n",
    "            average_training_mae = sum(training_maes) / len(training_maes)\n",
    "            average_training_losses.append(average_training_loss)\n",
    "            average_training_accs.append(average_training_acc)\n",
    "            average_training_maes.append(average_training_mae)\n",
    "\n",
    "            # Evaluation\n",
    "            val_accs = []\n",
    "            val_loss = []\n",
    "            val_maes = []\n",
    "            for x_val, y_val in dataloaders['val']:\n",
    "                try:\n",
    "                    acc, loss, mae = val_step(x_val, y_val, model, criterion)\n",
    "                except ValueError as ve:\n",
    "                    print(Fore.RED + ve + Fore.RESET)\n",
    "                    yaml_dict = {'error': str(ve) + ' during val: exploding gradient!'}\n",
    "                    with open(os.path.join(results_dir, 'results.yaml'), 'w') as yamlf:\n",
    "                        yaml.dump(yaml_dict, yamlf, default_flow_style=False, allow_unicode=True)\n",
    "                    return 0\n",
    "                \n",
    "                val_accs.append(acc)\n",
    "                val_loss.append(loss)\n",
    "                val_maes.append(mae)\n",
    "            average_val_loss = sum(val_loss) / len(val_loss)\n",
    "            average_val_acc = sum(val_accs) / len(val_accs)\n",
    "            average_val_mae = sum(val_maes)/len(val_maes)\n",
    "            average_val_accs.append(average_val_acc)\n",
    "            average_val_losses.append(average_val_loss)\n",
    "            average_val_maes.append(average_val_mae)\n",
    "            if best_val_accs < average_val_acc:\n",
    "                torch.save(model.state_dict(), os.path.join(results_dir, 'best.pt'))\n",
    "                best_val_accs = average_val_acc\n",
    "                best_epoch = epoch\n",
    "                print(Fore.GREEN + \"New best model: \", best_val_accs, Fore.RESET)\n",
    "            # Early stopping\n",
    "            if earlystop is not None:\n",
    "                if epoch > best_epoch+earlystop:\n",
    "                    early_stopping = epoch\n",
    "                    break\n",
    "            writer.writerow([epoch, average_training_loss, average_training_mae, average_training_acc, average_val_loss, average_val_mae, average_val_acc])\n",
    "\n",
    "    end_t = time.time()\n",
    "    time_elapsed=end_t - start_time\n",
    "    legend = ['train', 'val']\n",
    "    print(\"time_elapsed: {}\".format(str(datetime.timedelta(seconds=time_elapsed))))\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(20,10))\n",
    "    gs = GridSpec(3, 1, figure=fig, wspace = 0.1)\n",
    "    fig.suptitle('Results', fontsize=16)\n",
    "\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    intervals = np.arange(len(average_training_losses))\n",
    "    ax.plot(intervals, average_training_losses, color = 'b')\n",
    "    intervals = np.arange(1,len(average_training_losses))\n",
    "    ax.plot(intervals, average_val_losses[1:], color = 'g')\n",
    "    ax.set_ylabel(\"loss: \" + criterion._get_name())\n",
    "\n",
    "    ax = fig.add_subplot(gs[1])\n",
    "    intervals = np.arange(len(average_training_accs))\n",
    "    ax.plot(intervals, average_training_maes, color = 'b')\n",
    "    intervals = np.arange(1,len(average_training_accs))\n",
    "    ax.plot(intervals, average_val_maes[1:], color = 'g')\n",
    "    ax.set_ylabel(\"loss: MAE\")\n",
    "\n",
    "    ax = fig.add_subplot(gs[2])\n",
    "    intervals = np.arange(len(average_training_accs))\n",
    "    ax.plot(intervals, average_training_accs, color = 'b')\n",
    "    ax.plot(intervals, average_val_accs, color = 'g')\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "\n",
    "    fig.legend(legend)\n",
    "    fig.savefig(os.path.join(results_dir, 'results.png'))\n",
    "    plt.close()\n",
    "    if best_val_accs == 0 :\n",
    "        print(\"Evaluation accuracy (best) = \", best_val_accs)\n",
    "    elif best_val_accs > 0.9:\n",
    "        print(Back.GREEN + \"Evaluation accuracy (best) = \", best_val_accs)\n",
    "    elif best_val_accs > 0.8:\n",
    "        print(Back.BLUE + \"Evaluation accuracy (best) = \", best_val_accs)\n",
    "    elif best_val_accs > 0.65:\n",
    "        print('\\033[48;2;255;90;0m' + \"Evaluation accuracy (best) = \", best_val_accs)\n",
    "    else:\n",
    "        print(Back.RED + \"Evaluation accuracy (best) = \", best_val_accs)\n",
    "    print(Back.RESET)\n",
    "\n",
    "    # clear GPU cache memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Sauvegarde des résultats\n",
    "    yaml_dict = {'best epoch': best_epoch,\n",
    "                'best val acc': best_val_accs,\n",
    "                'train loss': average_training_losses[best_epoch],\n",
    "                'train acc': average_training_accs[best_epoch],\n",
    "                'train mae loss': average_training_maes[best_epoch],\n",
    "                'val loss': average_val_accs[best_epoch],\n",
    "                'val mae loss': average_val_maes[best_epoch],\n",
    "                'early_stopping': early_stopping,\n",
    "                'time': str(datetime.timedelta(seconds=time_elapsed))}\n",
    "    with open(os.path.join(results_dir, 'results.yaml'), 'w') as yamlf:\n",
    "        yaml.dump(yaml_dict, yamlf, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "    # Sauvegarde des matrices de confusion\n",
    "    path = os.path.join('dataset', 'val')\n",
    "    conf_mat, inds_p, inds_t = get_confusion_matrix(model, path, data_transforms)\n",
    "    inds_p = np.append(inds_p, 'other')\n",
    "    df_cm = pd.DataFrame(conf_mat, index = [i for i in inds_p],\n",
    "                    columns = [i for i in inds_t])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.ylabel('predicted')\n",
    "    plt.xlabel('ground truth')\n",
    "    plt.title('Val confusion matrix')\n",
    "    plt.savefig(os.path.join(results_dir, 'val_conf_mat.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # train\n",
    "    path = os.path.join('dataset', 'train')\n",
    "    conf_mat, inds_p, inds_t = get_confusion_matrix(model, path, data_transforms)\n",
    "    inds_p = np.append(inds_p, 'other')\n",
    "    df_cm = pd.DataFrame(conf_mat, index = [i for i in inds_p],\n",
    "                    columns = [i for i in inds_t])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.ylabel('predicted')\n",
    "    plt.xlabel('ground truth')\n",
    "    plt.title('train confusion matrix')\n",
    "    plt.savefig(os.path.join(results_dir, 'train_conf_mat.png'))\n",
    "    plt.close()\n",
    "\n",
    "    return best_val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(params):\n",
    "    \"\"\"Donne la fonction de perte (instance de la classe Loss)\n",
    "\n",
    "    Args:\n",
    "        params (str): Nom de la fonction\n",
    "\n",
    "    Returns:\n",
    "        criterion: Instance de la fonction de perte\n",
    "    \"\"\"\n",
    "    match params['loss_function']:\n",
    "        case 'MSE':\n",
    "            criterion = nn.MSELoss()\n",
    "        case 'MAE':\n",
    "            criterion = nn.L1Loss()\n",
    "        case 'Huber':\n",
    "            criterion = nn.HuberLoss(delta = params['delta'])\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, params):\n",
    "    \"\"\"Donne l'optimiseur (instance de la classe Optimizer)\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modèle\n",
    "        params (dict): Hyperparamètres\n",
    "\n",
    "    Returns:\n",
    "        optimizer: Instance de l'optimiseur\n",
    "    \"\"\"\n",
    "    lr = params['lr']\n",
    "    wd = params['weight_decay']\n",
    "    eps = params['epsilon']\n",
    "    rho = params['rho']\n",
    "    init = params['initial_accumulator_value']\n",
    "    betas = params['betas']\n",
    "    max_iter = params['max_iter']\n",
    "    md = params['momentum_decay']\n",
    "    alpha = params['alpha']\n",
    "    momentum = params['momentum']\n",
    "    \n",
    "    match params['optimizer']:\n",
    "        case 'SGD':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "        case 'Adadelta':\n",
    "            optimizer = torch.optim.Adadelta(model.parameters(), lr = lr, rho = rho, eps=eps, weight_decay=wd)\n",
    "        case 'Adagrad':\n",
    "            optimizer = torch.optim.Adagrad(model.parameters(), lr=lr, weight_decay=wd, eps=eps, initial_accumulator_value=init)\n",
    "        case 'RMSprop':\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=wd, eps=eps, alpha=alpha, momentum=momentum)\n",
    "        case 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd, eps=eps, betas=betas)\n",
    "        case 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, eps=eps, betas=betas)\n",
    "        case 'LBFGS':\n",
    "            optimizer = torch.optim.LBFGS(model.parameters(), lr=lr, max_iter=max_iter)\n",
    "        case 'NAdam':\n",
    "            optimizer = torch.optim.NAdam(model.parameters(), lr=lr, weight_decay=wd, eps=eps, betas=betas, momentum_decay=md)\n",
    "        case 'RAdam':\n",
    "            optimizer = torch.optim.RAdam(model.parameters(), lr=lr, weight_decay=wd, eps=eps, betas=betas)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(params):\n",
    "    \"\"\"Donne les Dataloaders (instance de la classe Dataloader)\n",
    "\n",
    "    Args:\n",
    "        params (dict): Dictionnaire des hyperparamètres\n",
    "\n",
    "    Returns:\n",
    "        dataloaders: Les dataloaders des differents splits (test, val,...)\n",
    "        data_transforms: Outils de pré-traitement des données\n",
    "    \"\"\"\n",
    "    data_dir = params['dataset']\n",
    "    batch_size = params['batch_size']\n",
    "    imgsz  = params['imgsz']\n",
    "    dataloaders = {}\n",
    "    splits = []\n",
    "    for split in ['test', 'val', 'train']:\n",
    "        if split in os.listdir(data_dir):\n",
    "            splits .append(split)\n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "            NewPad(imgsz = imgsz),\n",
    "            transforms.Resize((imgsz[1], imgsz[0])), # h,w\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])           \n",
    "     \n",
    "    for split in splits:\n",
    "        dataset = MyDataset(data_dir = data_dir, split = split, transform = data_transforms)\n",
    "        dataloaders[split] = DataLoader(dataset=dataset, shuffle=True, batch_size = batch_size, collate_fn = dataset.collate_fn)\n",
    "    return dataloaders, data_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_set(params):\n",
    "    \"\"\"Prépare et lance l'entrainement d'un modèle\n",
    "\n",
    "    Args:\n",
    "        params (dict): Dictionnaire des hyperparamètres\n",
    "\n",
    "    Do:\n",
    "        Sauvegarde les hyperparamètres \"args.yaml\"\n",
    "        Sauvegarde l'architecture du modèle \"model.txt\"\n",
    "\n",
    "    Returns:\n",
    "        float: Meilleur précision du meilleur modèle\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(params['results_dir'], 'results.yaml')):\n",
    "        print('Already trained')\n",
    "        with open(os.path.join(params['results_dir'], 'results.yaml'), 'r') as res:\n",
    "            res_data = yaml.safe_load(res)\n",
    "            try:\n",
    "                best_val_acc = res_data['best val acc']\n",
    "            except KeyError:\n",
    "                best_val_acc = 0\n",
    "            return best_val_acc\n",
    "    os.makedirs(params['results_dir'], exist_ok=True)\n",
    "    # Sauvegarde des paramètres\n",
    "    with open(os.path.join(params['results_dir'], 'args.yaml'), 'w') as yamlf:\n",
    "        yaml.dump(params, yamlf, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "    # Génération du modèle\n",
    "    model = gen_new_model(params['freeze'])\n",
    "    device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device) # déplacer le model vers le GPU a faire avant de construire l'optimiseur (pour adagrad)\n",
    "    optimizer = get_optimizer(model, params)\n",
    "    criterion = get_criterion(params)\n",
    "    dataloaders, data_transforms = get_dataloaders(params)\n",
    "    \n",
    "    # Sauvegarde de l'architecture du modèle\n",
    "    model_stats = summary(model, input_size=(params['batch_size'], 3, 224, 224), row_settings=(\"depth\", \"ascii_only\"))\n",
    "    summary_str = str(model_stats)\n",
    "    with open(os.path.join(params['results_dir'], 'model.txt'), 'w') as modelf:\n",
    "        modelf.write(summary_str)\n",
    "\n",
    "    # entrainement\n",
    "    train_params = {\n",
    "        'model': model,\n",
    "        'epochs': params['epochs'],\n",
    "        'earlystop': params['earlystop'],\n",
    "        'results_dir': params['results_dir'],\n",
    "        'dataloarders': dataloaders,\n",
    "        'optimizer': optimizer,\n",
    "        'criterion': criterion,\n",
    "        'data_transforms': data_transforms\n",
    "    }\n",
    "    best_acc = training_loop(train_params)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSchearch(grid):\n",
    "    \"\"\"Fait la recherche de grille (entraine un modèle pour chaque combinaison d'hyperparamètres)\n",
    "\n",
    "    Args:\n",
    "        grid (list: dict): Liste de dictionnaire correspondants aux combinaisons d'hyperparamètres\n",
    "\n",
    "    Returns:\n",
    "        best_params(dict): Dictionnaire avec les meilleurs hyperparamètres\n",
    "        best_acc(float): Meilleure précision pour la meilleure combinaisonn d'hyperparamètres\n",
    "        results(dict): Dictionnaire avec la meilleure précision pour chaque combinaison (chemin/des/resultats/de/la/combinaison: meilleure précision)\n",
    "    \"\"\"\n",
    "    # init\n",
    "    results = [('Path', 'Best Accuracy')]\n",
    "    best_acc = 0\n",
    "    best_params = {'None': None}\n",
    "    i = 0 # seulement pour l'affichage\n",
    "    for params_set_id in range(len(grid)):\n",
    "        i+=1\n",
    "        if i%5==0:\n",
    "            clear_output() # otherwise too many outputs\n",
    "        params_set = grid[params_set_id]\n",
    "        print(Fore.RED + '=========================================================={}/{}=========================================================='.format(params_set_id, len(grid)))\n",
    "        res = Test_set(params_set)\n",
    "        results.append((os.path.join(os.getcwd(), params_set['results_dir']), res))\n",
    "        if res>best_acc:\n",
    "            best_acc = res\n",
    "            best_params = params_set\n",
    "    best_params['best_acc'] = best_acc\n",
    "    return best_params, best_acc, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pdf(results, path, best):\n",
    "    \"\"\"Génère un pdf avec simplement un tableau récapitulatif des résultats pour chaque combinaison\n",
    "\n",
    "    Args:\n",
    "        results (dict): Resultats de la recherche en grille\n",
    "        path (str): Chemin + nom du fichier pdf\n",
    "        best (float): Valeur de la meilleure précision du meilleur modèle\n",
    "    \"\"\"\n",
    "    # create document\n",
    "    pdf = Document()\n",
    "    \n",
    "    # add page\n",
    "    page = Page(Decimal(1500), Decimal(1684))\n",
    "    m = Decimal(10)\n",
    "    pdf.add_page(page)\n",
    "    layout = SingleColumnLayoutWithOverflow(page)\n",
    "    colors = {0: X11Color(\"Red\"),\n",
    "                0.65: X11Color(\"Orange\"),\n",
    "                0.8: X11Color(\"Blue\"),\n",
    "                0.90: X11Color(\"Green\")}\n",
    "    for i in range(0, len(results), 38):\n",
    "        res = results[i:min(i+38, len(results))]\n",
    "        table = FlexibleColumnWidthTable(number_of_rows=len(res), number_of_columns=2)\n",
    "        for name, v in [(k,v) for k,v in res]:\n",
    "            if isinstance(v, str):\n",
    "                table.add(TableCell(Paragraph(text=name, font=\"Helvetica-Bold\", font_size=Decimal(12))))\n",
    "                table.add(TableCell(Paragraph(text=v, font=\"Helvetica-Bold\", font_size=Decimal(12))))\n",
    "                continue\n",
    "            table.add(TableCell(Paragraph(name)))\n",
    "            c = X11Color(\"Black\")\n",
    "            for b,bc in colors.items():\n",
    "                if v > b:\n",
    "                    c = bc\n",
    "                if v == best:\n",
    "                        c = X11Color('Purple')\n",
    "            table.add(            TableCell(\n",
    "                    Paragraph(str(v), horizontal_alignment=Alignment.CENTERED),\n",
    "                    background_color=c\n",
    "                ))\n",
    "        table.set_padding_on_all_cells(Decimal(m), Decimal(m), Decimal(m), Decimal(m))\n",
    "        # set border\n",
    "        table.set_border_width_on_all_cells(Decimal(0.2))\n",
    "        layout.add(table)\n",
    "    with open(path, \"wb\") as in_file_handle:\n",
    "        PDF.dumps(in_file_handle, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Génère la grille de recherche.\n",
    "    Appelle la fonction de recherche.\n",
    "    Génère le pdf de bilan.\n",
    "    \"\"\"\n",
    "    grid = make_grid()\n",
    "    best_params, best_acc, results = GridSchearch(grid)\n",
    "    print(Style.RESET_ALL)\n",
    "    print(Back.MAGENTA + f'Best accuracy from run: {best_acc}' + Back.RESET)\n",
    "    print(Fore.MAGENTA + 'Params:',best_params, Fore.RESET)\n",
    "    gen_pdf(results, os.path.join(grid_results_dir, results_pdf), best_params['best_acc'])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
